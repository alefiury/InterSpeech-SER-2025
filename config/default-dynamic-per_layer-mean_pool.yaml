title: SER-Dynamic-{model.model_name}-(steps-${trainer.max_epochs})-(bs-${train.batch_size})-(LR-${optimizer.params.learning_rate})-(wd-${optimizer.params.weight_decay})-Pretraining

wandb_entity: alefiury

datasets:
    train:
        - name: Train
          metadata_path: "/raid/alefiury/SER/InterSpeech2025/ravdess_train.csv"
          base_dir: "/raid/alefiury/SER/InterSpeech2025/Audio_Speech_Actors_01-24"
          filename_column: "filename"
          target_column: "emotion"

    val:
        - name: Validation
          metadata_path: "/raid/alefiury/SER/InterSpeech2025/ravdess_val.csv"
          base_dir: "/raid/alefiury/SER/InterSpeech2025/Audio_Speech_Actors_01-24"
          filename_column: "filename"
          target_column: "emotion"

data:
    num_classes: 8

    target_sr: 16000

    mixup_alpha: 0.0 # 0.0 to disable
    use_rand_truncation: false
    min_duration: 2.0 # min duration (in seconds) only used if use_rand_truncation is True
    min_white_noise_amp: 0.01 # min amplitude of white noise to be added
    max_white_noise_amp: 0.1 # max amplitude of white noise to be added
    insert_white_noise: false

    label2id: {
        "neutral": 0,
        "calm": 1,
        "happy": 2,
        "sad": 3,
        "angry": 4,
        "fearful": 5,
        "disgust": 6,
        "surprised": 7
    }

train:
    batch_size: 64
    shuffle: True
    num_workers: 12

model:
    model_type: "dynamic"
    # model_name: "openai/whisper-large"
    model_name: "microsoft/wavlm-large"
    # model_name: "facebook/wav2vec2-xls-r-300m"
    # model_name: "facebook/hubert-large-ls960-ft"
    mlp_input_dim: 1024
    mlp_hidden_dim: 1024
    mlp_num_layers: 2
    mlp_output_size: ${data.num_classes}
    mlp_dropout: 0.1
    mlp_activation_func: "relu"
    layer_weight_strategy: "per_layer"
    num_feature_layers: 25
    specific_layer_idx: -1
    pooling_strategy: "mean"

optimizer:
    name: "adamw"
    params:
        min_learning_rate: 1e-4
        learning_rate: 5e-4
        eps: 1e-8
        weight_decay: 1e-6
        betas: [0.9, 0.98]

scheduler:
    name: "CosineWarmupLR"
    params:
        warmup_lr: 500

tags:
    - ${train.batch_size}-BS
    - LR-${optimizer.params.learning_rate}
    - WD-${optimizer.params.weight_decay}
    - ${model.model_name}-HS
    - ${model.mlp_input_dim}-AH
    - ${model.mlp_hidden_dim}-HL
    - ${model.mlp_num_layers}-IS

trainer:
    accelerator: "gpu"
    max_epochs: 100
    num_sanity_val_steps: 2
    overfit_batches: 0.0
    log_every_n_steps: 10
    gradient_clip_val: 10.0
    gradient_clip_algorithm: "norm"
    val_check_interval: 1.0 # 1 epoch
    accumulate_grad_batches: 1

model_checkpoint:
    mode: "max"
    save_last: true
    save_weights_only: true
    monitor: "val/f1-score"
    dirpath: "../checkpoints/ser-2025"
    filename: "{epoch:02d}-{step:02d}-{val/f1-score:.4f}"